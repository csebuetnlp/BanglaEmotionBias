{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tripto et al Data - Analysis and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_length = 6\n",
    "\n",
    "file_path = \"archive/Emotion.csv\" # path_to_data_file\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path, on_bad_lines=\"warn\", delimiter=\";\")\n",
    "\n",
    "# Filter rows where the language is Bengali\n",
    "bn_df = df[df[\"lan\"] == \"BN\"]\n",
    "\n",
    "# Filter rows with more than allowed_length words\n",
    "length_filtered = bn_df[bn_df[\"text\"].apply(lambda x: len(x.split()) > allowed_length)]\n",
    "\n",
    "# Remove the 'lan' column\n",
    "length_filtered = length_filtered.drop(columns=[\"lan\"])\n",
    "\n",
    "# Remove duplicates based on the 'text' column\n",
    "length_filtered_unique_text_df = length_filtered.drop_duplicates(subset=\"text\")\n",
    "\n",
    "# Remove leading and trailing newline characters from the 'text' column\n",
    "length_filtered_unique_text_df[\"text\"] = length_filtered_unique_text_df[\"text\"].apply(lambda x: x.strip(\"\\n\"))\n",
    "\n",
    "# Remove rows where the 'emotion' column is \"none\"\n",
    "length_filtered_unique_text_df = length_filtered_unique_text_df[length_filtered_unique_text_df[\"emotion\"] != \"none\"]\n",
    "\n",
    "# Filter out rows with English letters using regular expression pattern\n",
    "length_filtered_unique_text_df = length_filtered_unique_text_df[length_filtered_unique_text_df[\"text\"].apply(lambda x: not bool(re.search(r'[A-Za-z]', x)))]\n",
    "\n",
    "# Print the count of selected unique text entries and their emotion value counts\n",
    "print(f\"Selected {len(length_filtered_unique_text_df)} unique text entries with more than {allowed_length} words\")\n",
    "print(length_filtered_unique_text_df[\"emotion\"].value_counts())\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "length_filtered_unique_text_df.to_csv(\"archive_tripto.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BN_Emo_Dataset Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    \"\"\"Read text from a file.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "def parse_text_lines(lines):\n",
    "    \"\"\"Parse text lines and create dictionaries.\"\"\"\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        # Split the line into emotion and text\n",
    "        emotion, text = line.strip().split(\" \", 1)\n",
    "        # Create a dictionary and append it to the list\n",
    "        data.append({\"emotion\": emotion, \"text\": text})\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "file_path = \"BN_Emo_Data/corpus_all.txt\"\n",
    "lines = read_text_file(file_path)\n",
    "data = parse_text_lines(lines)\n",
    "print(data[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df = df.drop_duplicates(subset=\"text\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_filtered_df = df[df[\"text\"].apply(lambda x: len(x.split()) > 5 and len(x.split()) < 18)]\n",
    "length_filtered_df[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_filtered_df.to_csv(\"./bn_emo_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AACL PAPER Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the 3 files of the dataset: test, train, val\n",
    "train_file_path = \"2022.aacl-short.17.Dataset/EmoNoBa Dataset/Train.csv\"\n",
    "val_file_path = \"2022.aacl-short.17.Dataset/EmoNoBa Dataset/Val.csv\"\n",
    "test_file_path = \"2022.aacl-short.17.Dataset/EmoNoBa Dataset/Test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_file_path)\n",
    "df_val = pd.read_csv(val_file_path)\n",
    "df_test = pd.read_csv(test_file_path)\n",
    "\n",
    "df = pd.concat([df_train, df_val, df_test])\n",
    "\n",
    "df = df.drop_duplicates(subset=\"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the emotion with the highest value for each row\n",
    "emotions = df[['Love', 'Joy', 'Surprise', 'Anger', 'Sadness', 'Fear']].idxmax(axis=1)\n",
    "\n",
    "# Map the column names to the corresponding emotions\n",
    "emotion_names = {'Love': 'Love', 'Joy': 'Joy', 'Surprise': 'Surprise', 'Anger': 'Anger', 'Sadness': 'Sadness', 'Fear': 'Fear'}\n",
    "emotions = emotions.map(emotion_names)\n",
    "\n",
    "# Add a new column named 'emotion'\n",
    "df['emotion'] = emotions\n",
    "\n",
    "# Drop the previous columns of emotions and additional columns\n",
    "df = df.drop(columns=['Love', 'Joy', 'Surprise', 'Anger', 'Sadness', 'Fear', 'is_admin'])\n",
    "\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter based on length criteria\n",
    "length_filtered_df = df[df[\"Data\"].apply(lambda x: 8 <= len(x.split()) < 18)]\n",
    "\n",
    "# Remove duplicates\n",
    "length_filtered_df = length_filtered_df.drop_duplicates(subset=\"Data\")\n",
    "\n",
    "# Get 10% of data with length less than 8\n",
    "less_than_8_df = df[df[\"Data\"].apply(lambda x: 4 <= len(x.split()) < 8)].sample(frac=0.1)\n",
    "print(len(less_than_8_df))\n",
    "\n",
    "# Get the remaining 90% of data from length_filtered_df\n",
    "greater_than_equal_to_8_df = length_filtered_df.sample(frac=0.7)\n",
    "\n",
    "# Concatenate both DataFrames\n",
    "filtered_df = pd.concat([less_than_8_df, greater_than_equal_to_8_df])\n",
    "\n",
    "# Shuffle the final DataFrame\n",
    "filtered_df = filtered_df.sample(frac=1)\n",
    "print(len(filtered_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df[~filtered_df[\"Data\"].str.contains(\"http\")]\n",
    "filtered_df[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.apply(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total DataPoints: {len(filtered_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[\"text\"] = filtered_df[\"Data\"]\n",
    "filtered_df = filtered_df.drop(columns=[\"Data\"])\n",
    "filtered_df.to_csv(\"aacl_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"merged_dataset.csv\")\n",
    "filtered_df = df[~df[\"text\"].str.contains(\"http\")]\n",
    "print(f\"Removed {len(df) - len(filtered_df)} rows with URLs\")\n",
    "filtered_df[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the Gen Dataset to Origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "final_dataset_filepath = \"\"\n",
    "final_dataset = pd.read_csv(final_dataset_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = '../../Storage'\n",
    "pattern = 'woman*'\n",
    "\n",
    "# Use glob.glob() to get a list of files matching the pattern\n",
    "matching_files = glob.glob(directory_path + '/*/' + pattern)\n",
    "\n",
    "# Now matching_files contains a list of file paths matching the pattern in the specified directory\n",
    "print(len(matching_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in matching_files:\n",
    "    lines = read_file(file_path)\n",
    "    if len(lines) > 1:\n",
    "        print(file_path)\n",
    "        print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the raw response data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "raw_dataset_path = \"\" # the comments dataset\n",
    "storage_path = \"\" # path where the LLM responses are stored\n",
    "model_name = \"\" # name of the model of inference, consistent witht he column name like gpt_4o\n",
    "saved_csv_filename = \"\" # name of the saved csv file e.g. gpt_4o_dataframe_i2_raw.csv\n",
    "\n",
    "# Read the original DataFrame\n",
    "df = pd.read_csv(raw_dataset_path)\n",
    "\n",
    "# Define a function to read the response files\n",
    "def read_response_files(row):\n",
    "    id_folder = str(row['ID'])\n",
    "    if os.path.exists(os.path.join(storage_path, id_folder)):\n",
    "        print(f\"Processing ID: {id_folder}\")\n",
    "    else:\n",
    "        print(f\"Folder not found for ID: {id_folder}\")\n",
    "        return row\n",
    "    man_file_path = os.path.join(storage_path, id_folder, f\"man_{model_name}_response.txt\")\n",
    "    woman_file_path = os.path.join(storage_path, id_folder, f\"woman_{model_name}_response.txt\")\n",
    "    \n",
    "    # Initialize response variables\n",
    "    man_response = \"\"\n",
    "    woman_response = \"\"\n",
    "    \n",
    "    # Read man response file\n",
    "    if os.path.exists(man_file_path):\n",
    "        with open(man_file_path, 'r', encoding='utf-8') as man_file:\n",
    "            man_response = man_file.read()\n",
    "    else:\n",
    "        print(\"man response not found\")\n",
    "    \n",
    "    # Read woman response file\n",
    "    if os.path.exists(woman_file_path):\n",
    "        with open(woman_file_path, 'r', encoding='utf-8') as woman_file:\n",
    "            woman_response = woman_file.read()\n",
    "    else:\n",
    "        print(\"women response not found\")\n",
    "    \n",
    "    # Update the row with response data\n",
    "    row[f\"man_{model_name}_response\"] = man_response\n",
    "    row[f\"woman_{model_name}_response\"] = woman_response\n",
    "    \n",
    "    return row\n",
    "\n",
    "# Apply the function to each row of the DataFrame\n",
    "df_with_responses = df.apply(read_response_files, axis=1)\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "df_with_responses.to_csv(saved_csv_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the multiple lines response from dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename = \"\" # the file containing responses\n",
    "response_df = pd.read_csv(filename)\n",
    "response_df.fillna(\"\", inplace=True)\n",
    "response_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender = \"man\"\n",
    "gender = \"woman\"\n",
    "# column = f\"{gender}_llama_3_response\"\n",
    "column = f\"{gender}_gpt_3_5_turbo_response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emotion_words = response_df[column].unique().tolist()\n",
    "print(f\"Analyzing for {gender} Emotional Attributes\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Total Number of Unique values: {len(emotion_words)}\")\n",
    "print(f\"Value Counts:\\n\")\n",
    "values = response_df[column].value_counts().to_dict()\n",
    "for k, v in values.items():\n",
    "    print(f\"\\\"{k}\\\": {v},\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalizer import normalize\n",
    "# replacement_column = \"man_gpt_3_5_turbo_response\"\n",
    "replacement_column = column\n",
    "# replacement_column = \"woman_gpt_3_5_turbo_response\"\n",
    "\n",
    "def strip_word(row):\n",
    "    return normalize(row[replacement_column].strip())\n",
    "\n",
    "def remove_end_marks(row):\n",
    "    return row[replacement_column].strip().rstrip(\"।\").rstrip(\"!\").rstrip(\".\").strip(\"\\\"\")\n",
    "\n",
    "response_df[replacement_column] = response_df.apply(strip_word, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_words = []\n",
    "for words in man_emotion_words:\n",
    "    if len(words.split()) > 1:\n",
    "        print(words)\n",
    "        long_words.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman_long_words = []\n",
    "for words in woman_emotion_words:\n",
    "    if len(words.split()) > 1:\n",
    "        print(words)\n",
    "        woman_long_words.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the dataset to have a single response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalizer import normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_file_path = \"\" # pth to the file where substitutes are placed\n",
    "with open(change_file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "print(f\"total lines read: {len(lines)}\")\n",
    "replacement_dict= {}\n",
    "for line in lines:\n",
    "    line = line.strip(\"\\n\")\n",
    "    replacement_dict[line.split(\"->\")[0]] = normalize(line.split(\"->\")[1].strip())\n",
    "print(len(replacement_dict.keys()))\n",
    "replacement_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = \"\" # the file containing responses of models merged with actual dataset\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.fillna('')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace values\n",
    "dict_keys = replacement_dict.keys()\n",
    "normal_dict = {}\n",
    "for key in dict_keys:\n",
    "    normal_dict[key] = normalize(key).strip() \n",
    "\n",
    "replacement_column = 'man_gpt_3_5_turbo_response'\n",
    "\n",
    "# replacement_column = 'woman_gpt_3_5_turbo_response'\n",
    "\n",
    "def replace_values(row):\n",
    "    stripped_value = normalize(row[replacement_column].strip())\n",
    "    for line in dict_keys:\n",
    "        normal_line = normal_dict[line]\n",
    "        if (stripped_value == normal_line):\n",
    "            print(f\"Corrected: {stripped_value} (replacement: {replacement_dict[line]})\")\n",
    "            return replacement_dict[line]\n",
    "    splitted_value = stripped_value.split()\n",
    "    if len(splitted_value) > 1:\n",
    "        if len(splitted_value) == 2 and splitted_value[-1] == '।':\n",
    "            return splitted_value[0]\n",
    "        print(f\"Not found: {stripped_value}\")\n",
    "    return row[replacement_column]\n",
    "\n",
    "# Apply the function to the specified column\n",
    "df[replacement_column] = df.apply(replace_values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatgpt use to clean the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_not_found_path = \"\" # the instances where data is not found for replacement\n",
    "with open(subs_not_found_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "segment_length = 40\n",
    "all_data = []\n",
    "total_data_points = 0\n",
    "for i in range(len(lines)//segment_length+1):\n",
    "    segment = lines[i*segment_length:(i+1)*segment_length]\n",
    "    total_data_points += len(segment)\n",
    "    all_data.append(segment)\n",
    "\n",
    "print(f\"Total data points: {total_data_points}\")\n",
    "assert(total_data_points == len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the prompts\n",
    "prompts = []\n",
    "for i in range(len(all_data)):\n",
    "    prompt = \";\".join(all_data[i])\n",
    "    prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruct = '''You are good at parsing Bangla. I am going to give you some Bangla phrases that expresses some emotions. You will be given the sentences with ; as delimeter.\n",
    "Your work is to extract the emotion from the phrase and express it in ONE WORD. Remember the emotion might not be explicitly mentioned always. In those cases infer it from the phrase.\n",
    "You can select from the following emotions: রাগ, দুঃখ, আনন্দ, বিস্ময়, ভয়, অপরাধবোধ, বিরক্তি, লজ্জা but you can also use other words if you think the emotion is not in the list.\n",
    "Input is like this: আমি ব্যস্ত।;আমার জন্য আদর্শ এবং শ্রদ্ধা।;শহীদ্গঞ্জার সাহসী ।\n",
    "Output is like:''' \n",
    "output = '''আমি ব্যস্ত।->ব্যস্ত;\n",
    "আমার জন্য আদর্শ এবং শ্রদ্ধা।->শ্রদ্ধা(note that আদর্শ is not an emotion word);\n",
    "শহীদ্গঞ্জার সাহসী ।->আনন্দ'''\n",
    "\n",
    "model_message = [\n",
    "    {\"role\": \"system\", \"content\": system_instruct.replace(\"\\n\", \" \")+output},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "model_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo\"\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"\") # insert your api key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, prompt in enumerate(prompts):\n",
    "    if i < 4:\n",
    "        continue\n",
    "    print(f\"Processing segment {i+1}\")\n",
    "    model_message = [\n",
    "        {\"role\": \"system\", \"content\": system_instruct.replace(\"\\n\", \" \")+output},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    print(model_message)\n",
    "    completion = client.chat.completions.create(\n",
    "                model=model_name, messages=model_message\n",
    "            )\n",
    "    print(completion.choices[0].message)\n",
    "    folder_path = \"\" # directory to save the data\n",
    "    with open(f\"{folder_path}/chatresponse{i}\", \"w\") as file:\n",
    "        file.writelines(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_response_file_path = \"\" # file path to \n",
    "with open(subs_response_file_path, \"r\") as file:\n",
    "    line = file.readline()\n",
    "    print(line)\n",
    "\n",
    "responses = line.split(\";\")\n",
    "len(responses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the unique words for male and female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename = \"\" # the file containing responses of models merged with actual dataset\n",
    "response_df = pd.read_csv(filename)\n",
    "response_df.fillna(\"\", inplace=True)\n",
    "response_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = \"man\"\n",
    "gender2 = \"woman\"\n",
    "# column = f\"{gender}_llama_3_response\"\n",
    "male_column = f\"{gender}_gpt_3_5_turbo_response\"\n",
    "female_column = f\"{gender2}_gpt_3_5_turbo_response\"\n",
    "\n",
    "# male_column = f\"man_llama_3_response\"\n",
    "# female_column = f\"woman_llama_3_response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_emotion_words = response_df[male_column].unique().tolist()\n",
    "male_emotion_value_count = response_df[male_column].value_counts()\n",
    "\n",
    "female_emotion_words = response_df[female_column].unique().tolist()\n",
    "female_emotion_value_count = response_df[female_column].value_counts()\n",
    "\n",
    "men_unique_words = []\n",
    "female_unique_words = []\n",
    "\n",
    "for word in male_emotion_words:\n",
    "    if word not in female_emotion_words:\n",
    "        men_unique_words.append((word, male_emotion_value_count[word]))\n",
    "\n",
    "for word in female_emotion_words:\n",
    "    if word not in male_emotion_words:\n",
    "        female_unique_words.append((word, female_emotion_value_count[word]))\n",
    "\n",
    "men_unique_words.sort(key=lambda x: x[1], reverse=True)\n",
    "female_unique_words.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"Chatgpt v2\")\n",
    "print(\"The words that are available in male but not available in female emotion words:\")\n",
    "for word, count in men_unique_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"The words that are available in female but not available in male emotion words:\")\n",
    "for word, count in female_unique_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find word distribution for each Gendered response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_filename = \".Final_Versions/chatgpt_dataframe_v2.csv\"\n",
    "\n",
    "#llama3_v1\n",
    "csv_filename = \"\" # the response filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Create a dictionary to store emotion counts for male responses\n",
    "male_emotion_counts = df['man_gpt_3_5_turbo_response'].value_counts()\n",
    "\n",
    "# Create a dictionary to store the top emotions for each emotion in male responses\n",
    "top_emotions_for_each_emotion = {}\n",
    "\n",
    "# Loop through each unique male emotion\n",
    "for emotion in df['man_gpt_3_5_turbo_response'].unique():\n",
    "    # Filter the dataframe for the current male emotion\n",
    "    filtered_df = df[df['man_gpt_3_5_turbo_response'] == emotion]\n",
    "    \n",
    "    # Count occurrences of female emotions in responses corresponding to the current male emotion\n",
    "    female_emotion_counts = filtered_df['woman_gpt_3_5_turbo_response'].value_counts().head(7)\n",
    "    \n",
    "    # Store the top female emotions for the current male emotion\n",
    "    top_emotions_for_each_emotion[emotion] = female_emotion_counts\n",
    "\n",
    "# Output the results\n",
    "for male_emotion, count in male_emotion_counts.items():\n",
    "    top_emotions_str = \", \".join([f\"{female_emotion}({female_count})\" for female_emotion, female_count in top_emotions_for_each_emotion.get(male_emotion, {}).items()])\n",
    "    print(f\"{male_emotion}({count}): {top_emotions_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for woman\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Create a dictionary to store emotion counts for male responses\n",
    "female_emotion_counts = df['woman_gpt_3_5_turbo_response'].value_counts()\n",
    "\n",
    "# Create a dictionary to store the top emotions for each emotion in male responses\n",
    "top_emotions_for_each_emotion = {}\n",
    "\n",
    "# Loop through each unique male emotion\n",
    "for emotion in df['woman_gpt_3_5_turbo_response'].unique():\n",
    "    # Filter the dataframe for the current male emotion\n",
    "    filtered_df = df[df['woman_gpt_3_5_turbo_response'] == emotion]\n",
    "    \n",
    "    # Count occurrences of female emotions in responses corresponding to the current male emotion\n",
    "    male_emotion_counts = filtered_df['man_gpt_3_5_turbo_response'].value_counts().head(7)\n",
    "    \n",
    "    # Store the top female emotions for the current male emotion\n",
    "    top_emotions_for_each_emotion[emotion] = male_emotion_counts\n",
    "\n",
    "# Output the results\n",
    "for female_emotion, count in female_emotion_counts.items():\n",
    "    top_emotions_str = \", \".join([f\"{male_emotion}({male_count})\" for male_emotion, male_count in top_emotions_for_each_emotion.get(female_emotion, {}).items()])\n",
    "    print(f\"{female_emotion}({count}): {top_emotions_str}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Create a dictionary to store emotion counts for male responses\n",
    "male_emotion_counts = df['man_llama_3_response'].value_counts()\n",
    "\n",
    "# Create a dictionary to store the top emotions for each emotion in male responses\n",
    "top_emotions_for_each_emotion = {}\n",
    "\n",
    "# Loop through each unique male emotion\n",
    "for emotion in df['man_llama_3_response'].unique():\n",
    "    # Filter the dataframe for the current male emotion\n",
    "    filtered_df = df[df['man_llama_3_response'] == emotion]\n",
    "    \n",
    "    # Count occurrences of female emotions in responses corresponding to the current male emotion\n",
    "    female_emotion_counts = filtered_df['woman_llama_3_response'].value_counts().head(7)\n",
    "    \n",
    "    # Store the top female emotions for the current male emotion\n",
    "    top_emotions_for_each_emotion[emotion] = female_emotion_counts\n",
    "\n",
    "# Output the results\n",
    "for male_emotion, count in male_emotion_counts.items():\n",
    "    top_emotions_str = \", \".join([f\"{female_emotion}({female_count})\" for female_emotion, female_count in top_emotions_for_each_emotion.get(male_emotion, {}).items()])\n",
    "    print(f\"{male_emotion}({count}): {top_emotions_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for woman\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Create a dictionary to store emotion counts for male responses\n",
    "female_emotion_counts = df['woman_llama_3_response'].value_counts()\n",
    "\n",
    "# Create a dictionary to store the top emotions for each emotion in male responses\n",
    "top_emotions_for_each_emotion = {}\n",
    "\n",
    "# Loop through each unique male emotion\n",
    "for emotion in df['woman_llama_3_response'].unique():\n",
    "    # Filter the dataframe for the current male emotion\n",
    "    filtered_df = df[df['woman_llama_3_response'] == emotion]\n",
    "    \n",
    "    # Count occurrences of female emotions in responses corresponding to the current male emotion\n",
    "    male_emotion_counts = filtered_df['man_llama_3_response'].value_counts().head(7)\n",
    "    \n",
    "    # Store the top female emotions for the current male emotion\n",
    "    top_emotions_for_each_emotion[emotion] = male_emotion_counts\n",
    "\n",
    "# Output the results\n",
    "for female_emotion, count in female_emotion_counts.items():\n",
    "    top_emotions_str = \", \".join([f\"{male_emotion}({male_count})\" for male_emotion, male_count in top_emotions_for_each_emotion.get(female_emotion, {}).items()])\n",
    "    print(f\"{female_emotion}({count}): {top_emotions_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
